{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "617a21a9",
   "metadata": {
    "papermill": {
     "duration": 0.038625,
     "end_time": "2022-02-25T11:43:10.972504",
     "exception": false,
     "start_time": "2022-02-25T11:43:10.933879",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Table of Contents\n",
    "\n",
    "* Introduction to Data Scraping\n",
    "* Data Scraping from Twitter\n",
    "* Data Scraping from Reddit\n",
    "* Acknowledgements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6fc937",
   "metadata": {
    "papermill": {
     "duration": 0.03649,
     "end_time": "2022-02-25T11:43:11.046687",
     "exception": false,
     "start_time": "2022-02-25T11:43:11.010197",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Introduction to Data Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ae6f3b",
   "metadata": {
    "papermill": {
     "duration": 0.035872,
     "end_time": "2022-02-25T11:43:11.118915",
     "exception": false,
     "start_time": "2022-02-25T11:43:11.083043",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Data is available everywhere. To perform various Data Science experiments we often need to extract data from various sources. \n",
    "\n",
    "We will use the codes in this notebook to extract data from some of the popular websites (Twitter and Reddit). The codes published here can be used to extract tweets based on the user's requirement and converted to a Pandas dataset. For Reddit, you can scrape an entire subreddit and convert it into a Pandas dataset.\n",
    "\n",
    "### Method 3 for Twitter uses snscrape which requires Python version 3.8. As Kaggle notebooks run on Python version 3.7 and I could not find a reliable work around on this, the codes for Method 3 have been commented to avoid error on execution. \n",
    "\n",
    "### Please use Python version 3.8 or higher on Jupyter notebooks to run the codes for Twitter Method 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcdee5a",
   "metadata": {
    "papermill": {
     "duration": 0.035575,
     "end_time": "2022-02-25T11:43:11.190697",
     "exception": false,
     "start_time": "2022-02-25T11:43:11.155122",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data Scraping from Twitter\n",
    "\n",
    "### You need a Twitter Developer account for Method 1 and 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c80ed3",
   "metadata": {
    "papermill": {
     "duration": 0.044239,
     "end_time": "2022-02-25T11:43:11.271708",
     "exception": false,
     "start_time": "2022-02-25T11:43:11.227469",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Method 1 - Twitter Scraper using Keywords\n",
    "\n",
    "##### Extract all tweets based on a keyword, e.g. Covid-19, DataScience, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2429823d",
   "metadata": {
    "papermill": {
     "duration": 0.035791,
     "end_time": "2022-02-25T11:43:11.348414",
     "exception": false,
     "start_time": "2022-02-25T11:43:11.312623",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 1. Install Tweepy and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22afda1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T11:43:11.432361Z",
     "iopub.status.busy": "2022-02-25T11:43:11.431892Z",
     "iopub.status.idle": "2022-02-25T11:43:20.838252Z",
     "shell.execute_reply": "2022-02-25T11:43:20.837732Z",
     "shell.execute_reply.started": "2021-10-17T08:30:55.533045Z"
    },
    "papermill": {
     "duration": 9.453056,
     "end_time": "2022-02-25T11:43:20.838392",
     "exception": false,
     "start_time": "2022-02-25T11:43:11.385336",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tweepy\r\n",
      "  Downloading tweepy-4.6.0-py2.py3-none-any.whl (69 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 69 kB 531 kB/s \r\n",
      "\u001b[?25hCollecting oauthlib<4,>=3.2.0\r\n",
      "  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 151 kB 986 kB/s \r\n",
      "\u001b[?25hRequirement already satisfied: requests-oauthlib<2,>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from tweepy) (1.3.0)\r\n",
      "Collecting requests<3,>=2.27.0\r\n",
      "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 63 kB 888 kB/s \r\n",
      "\u001b[?25hRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.27.0->tweepy) (2.10)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.27.0->tweepy) (1.26.6)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.27.0->tweepy) (2021.5.30)\r\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.27.0->tweepy) (2.0.4)\r\n",
      "Installing collected packages: requests, oauthlib, tweepy\r\n",
      "  Attempting uninstall: requests\r\n",
      "    Found existing installation: requests 2.25.1\r\n",
      "    Uninstalling requests-2.25.1:\r\n",
      "      Successfully uninstalled requests-2.25.1\r\n",
      "  Attempting uninstall: oauthlib\r\n",
      "    Found existing installation: oauthlib 3.1.1\r\n",
      "    Uninstalling oauthlib-3.1.1:\r\n",
      "      Successfully uninstalled oauthlib-3.1.1\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "caip-notebooks-serverextension 1.0.0 requires google-cloud-bigquery-storage, which is not installed.\r\n",
      "jupyterlab-git 0.11.0 requires nbdime<2.0.0,>=1.1.0, but you have nbdime 3.1.0 which is incompatible.\r\n",
      "gcsfs 2021.7.0 requires fsspec==2021.07.0, but you have fsspec 2021.8.1 which is incompatible.\r\n",
      "earthengine-api 0.1.283 requires google-api-python-client<2,>=1.12.1, but you have google-api-python-client 1.8.0 which is incompatible.\u001b[0m\r\n",
      "Successfully installed oauthlib-3.2.0 requests-2.27.1 tweepy-4.6.0\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b21a854",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T11:43:20.926457Z",
     "iopub.status.busy": "2022-02-25T11:43:20.925926Z",
     "iopub.status.idle": "2022-02-25T11:43:21.026800Z",
     "shell.execute_reply": "2022-02-25T11:43:21.026304Z",
     "shell.execute_reply.started": "2021-10-17T08:31:04.667311Z"
    },
    "papermill": {
     "duration": 0.146657,
     "end_time": "2022-02-25T11:43:21.026914",
     "exception": false,
     "start_time": "2022-02-25T11:43:20.880257",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tweepy as tw\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6cc0bc",
   "metadata": {
    "papermill": {
     "duration": 0.043039,
     "end_time": "2022-02-25T11:43:21.113002",
     "exception": false,
     "start_time": "2022-02-25T11:43:21.069963",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 2. Twitter API Authentication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3821a19",
   "metadata": {
    "papermill": {
     "duration": 0.043072,
     "end_time": "2022-02-25T11:43:21.200471",
     "exception": false,
     "start_time": "2022-02-25T11:43:21.157399",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Pass in the CONSUMER_API_KEY and CONSUMER_API_SECRET from your Twitter Developer account. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01bf9ec3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T11:43:21.290942Z",
     "iopub.status.busy": "2022-02-25T11:43:21.290457Z",
     "iopub.status.idle": "2022-02-25T11:43:21.997946Z",
     "shell.execute_reply": "2022-02-25T11:43:21.998331Z",
     "shell.execute_reply.started": "2021-10-17T08:31:04.830672Z"
    },
    "papermill": {
     "duration": 0.752648,
     "end_time": "2022-02-25T11:43:21.998527",
     "exception": false,
     "start_time": "2022-02-25T11:43:21.245879",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "\n",
    "consumer_api_key = user_secrets.get_secret(\"CONSUMER_API_KEY\")\n",
    "consumer_api_secret = user_secrets.get_secret(\"CONSUMER_API_SECRET\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "692d0534",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T11:43:22.085870Z",
     "iopub.status.busy": "2022-02-25T11:43:22.085351Z",
     "iopub.status.idle": "2022-02-25T11:43:22.088931Z",
     "shell.execute_reply": "2022-02-25T11:43:22.089318Z",
     "shell.execute_reply.started": "2021-10-17T08:31:05.451718Z"
    },
    "papermill": {
     "duration": 0.048861,
     "end_time": "2022-02-25T11:43:22.089469",
     "exception": false,
     "start_time": "2022-02-25T11:43:22.040608",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "auth = tw.OAuthHandler(consumer_api_key, consumer_api_secret)\n",
    "api = tw.API(auth, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbf6688",
   "metadata": {
    "papermill": {
     "duration": 0.039967,
     "end_time": "2022-02-25T11:43:22.169925",
     "exception": false,
     "start_time": "2022-02-25T11:43:22.129958",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 3. Tweets Query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d1f372",
   "metadata": {
    "papermill": {
     "duration": 0.040593,
     "end_time": "2022-02-25T11:43:22.251092",
     "exception": false,
     "start_time": "2022-02-25T11:43:22.210499",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 3.1 Define the Query\n",
    "\n",
    "In the below cell, we are collecting all (max=500) DataScience tweets since 1st Jan, 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6953b41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T11:43:22.335083Z",
     "iopub.status.busy": "2022-02-25T11:43:22.334624Z",
     "iopub.status.idle": "2022-02-25T11:43:22.339175Z",
     "shell.execute_reply": "2022-02-25T11:43:22.339639Z",
     "shell.execute_reply.started": "2021-10-17T08:31:05.460421Z"
    },
    "papermill": {
     "duration": 0.048078,
     "end_time": "2022-02-25T11:43:22.339784",
     "exception": false,
     "start_time": "2022-02-25T11:43:22.291706",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "search_words = \"#datascience -filter:retweets\"\n",
    "date_since = \"2021-01-01\"\n",
    "# # Collect tweets\n",
    "tweets = tw.Cursor(api.search_tweets,\n",
    "              q=search_words,\n",
    "              lang=\"en\",\n",
    "              since=date_since).items(500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160a96cc",
   "metadata": {
    "papermill": {
     "duration": 0.041736,
     "end_time": "2022-02-25T11:43:22.422802",
     "exception": false,
     "start_time": "2022-02-25T11:43:22.381066",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 3.2 Retrieve the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bf43c2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T11:43:22.507033Z",
     "iopub.status.busy": "2022-02-25T11:43:22.506576Z",
     "iopub.status.idle": "2022-02-25T11:43:34.623087Z",
     "shell.execute_reply": "2022-02-25T11:43:34.622587Z",
     "shell.execute_reply.started": "2021-10-17T08:31:05.471946Z"
    },
    "papermill": {
     "duration": 12.159875,
     "end_time": "2022-02-25T11:43:34.623224",
     "exception": false,
     "start_time": "2022-02-25T11:43:22.463349",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500it [00:12, 41.33it/s]\n"
     ]
    }
   ],
   "source": [
    "tweets_copy = []\n",
    "for tweet in tqdm(tweets):\n",
    "     tweets_copy.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "684c772c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T11:43:34.732613Z",
     "iopub.status.busy": "2022-02-25T11:43:34.731956Z",
     "iopub.status.idle": "2022-02-25T11:43:34.736135Z",
     "shell.execute_reply": "2022-02-25T11:43:34.735648Z",
     "shell.execute_reply.started": "2021-10-17T08:31:18.446221Z"
    },
    "papermill": {
     "duration": 0.061322,
     "end_time": "2022-02-25T11:43:34.736342",
     "exception": false,
     "start_time": "2022-02-25T11:43:34.675020",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new tweets retrieved: 500\n"
     ]
    }
   ],
   "source": [
    "print(f\"new tweets retrieved: {len(tweets_copy)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a4df4e",
   "metadata": {
    "papermill": {
     "duration": 0.05119,
     "end_time": "2022-02-25T11:43:34.838761",
     "exception": false,
     "start_time": "2022-02-25T11:43:34.787571",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 4. Populate the Dataset\n",
    "\n",
    "#### Extract the information contained in a tweet into a Pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc0b88eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T11:43:34.951145Z",
     "iopub.status.busy": "2022-02-25T11:43:34.950314Z",
     "iopub.status.idle": "2022-02-25T11:45:57.701577Z",
     "shell.execute_reply": "2022-02-25T11:45:57.700925Z",
     "shell.execute_reply.started": "2021-10-17T08:31:18.453851Z"
    },
    "papermill": {
     "duration": 142.811121,
     "end_time": "2022-02-25T11:45:57.701694",
     "exception": false,
     "start_time": "2022-02-25T11:43:34.890573",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [02:22<00:00,  3.50it/s]\n"
     ]
    }
   ],
   "source": [
    "tweets_df = pd.DataFrame()\n",
    "for tweet in tqdm(tweets_copy):\n",
    "    hashtags = []\n",
    "    try:\n",
    "        for hashtag in tweet.entities[\"hashtags\"]:\n",
    "            hashtags.append(hashtag[\"text\"])\n",
    "        text = api.get_status(id=tweet.id, tweet_mode='extended').full_text\n",
    "    except:\n",
    "        pass\n",
    "    tweets_df = tweets_df.append(pd.DataFrame({'user_name': tweet.user.name, \n",
    "                                               'user_location': tweet.user.location,\n",
    "                                               'user_description': tweet.user.description,\n",
    "                                               'user_created': tweet.user.created_at,\n",
    "                                               'user_followers': tweet.user.followers_count,\n",
    "                                               'user_friends': tweet.user.friends_count,\n",
    "                                               'user_favourites': tweet.user.favourites_count,\n",
    "                                               'user_verified': tweet.user.verified,\n",
    "                                               'date': tweet.created_at,\n",
    "                                               'text': text, \n",
    "                                               'hashtags': [hashtags if hashtags else None],\n",
    "                                               'source': tweet.source,\n",
    "                                               'is_retweet': tweet.retweeted}, index=[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551cb6ab",
   "metadata": {
    "papermill": {
     "duration": 0.177263,
     "end_time": "2022-02-25T11:45:58.059460",
     "exception": false,
     "start_time": "2022-02-25T11:45:57.882197",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Check head of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad4e946a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T11:45:58.541409Z",
     "iopub.status.busy": "2022-02-25T11:45:58.540713Z",
     "iopub.status.idle": "2022-02-25T11:45:58.565255Z",
     "shell.execute_reply": "2022-02-25T11:45:58.565840Z",
     "shell.execute_reply.started": "2021-10-17T08:33:48.567847Z"
    },
    "papermill": {
     "duration": 0.309543,
     "end_time": "2022-02-25T11:45:58.566006",
     "exception": false,
     "start_time": "2022-02-25T11:45:58.256463",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_name</th>\n",
       "      <th>user_location</th>\n",
       "      <th>user_description</th>\n",
       "      <th>user_created</th>\n",
       "      <th>user_followers</th>\n",
       "      <th>user_friends</th>\n",
       "      <th>user_favourites</th>\n",
       "      <th>user_verified</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>source</th>\n",
       "      <th>is_retweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Salynt</td>\n",
       "      <td>Washington, DC</td>\n",
       "      <td>Salynt provides data scientists and software e...</td>\n",
       "      <td>2021-08-11 19:50:54+00:00</td>\n",
       "      <td>47</td>\n",
       "      <td>20</td>\n",
       "      <td>54</td>\n",
       "      <td>False</td>\n",
       "      <td>2022-02-25 11:43:18+00:00</td>\n",
       "      <td>We're fundamentally changing how software deve...</td>\n",
       "      <td>[softwareengineering, datascience, AI]</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bharat</td>\n",
       "      <td>Mumbai, India</td>\n",
       "      <td>Data Enthusiast. NLP/Text Analytics.</td>\n",
       "      <td>2011-10-17 10:07:17+00:00</td>\n",
       "      <td>635</td>\n",
       "      <td>424</td>\n",
       "      <td>11177</td>\n",
       "      <td>False</td>\n",
       "      <td>2022-02-25 11:42:08+00:00</td>\n",
       "      <td>Practical Advice for R in Production -  Answer...</td>\n",
       "      <td>[Analytics, DataScience, AI, ML, RStats, Python]</td>\n",
       "      <td>NadarSenpai</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Saransh Inc</td>\n",
       "      <td>Plainsboro, New Jersey</td>\n",
       "      <td>We are a people-centric company dedicated towa...</td>\n",
       "      <td>2020-02-28 09:35:19+00:00</td>\n",
       "      <td>1968</td>\n",
       "      <td>1928</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>2022-02-25 11:41:23+00:00</td>\n",
       "      <td>Data can be simply defined as 'what you need t...</td>\n",
       "      <td>None</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nathan Joyner</td>\n",
       "      <td>Los Angeles, CA</td>\n",
       "      <td>Global Venture Captial and Private Equity/Busi...</td>\n",
       "      <td>2015-05-18 20:52:29+00:00</td>\n",
       "      <td>60</td>\n",
       "      <td>11</td>\n",
       "      <td>824</td>\n",
       "      <td>False</td>\n",
       "      <td>2022-02-25 11:40:58+00:00</td>\n",
       "      <td>Daily Confirmed Covid Cases per 1K Population ...</td>\n",
       "      <td>None</td>\n",
       "      <td>smcapplication</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Richard Eudes, PhD</td>\n",
       "      <td>Paris, France</td>\n",
       "      <td>Director @Deloitte. Long-time expert in #DataS...</td>\n",
       "      <td>2009-06-21 21:04:32+00:00</td>\n",
       "      <td>17923</td>\n",
       "      <td>1849</td>\n",
       "      <td>1472</td>\n",
       "      <td>False</td>\n",
       "      <td>2022-02-25 11:40:01+00:00</td>\n",
       "      <td>Design Patterns in Machine Learning for MLOps ...</td>\n",
       "      <td>[analytics, datascience, bigdata]</td>\n",
       "      <td>Buffer</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            user_name           user_location  \\\n",
       "0              Salynt          Washington, DC   \n",
       "0              Bharat           Mumbai, India   \n",
       "0         Saransh Inc  Plainsboro, New Jersey   \n",
       "0       Nathan Joyner         Los Angeles, CA   \n",
       "0  Richard Eudes, PhD           Paris, France   \n",
       "\n",
       "                                    user_description  \\\n",
       "0  Salynt provides data scientists and software e...   \n",
       "0               Data Enthusiast. NLP/Text Analytics.   \n",
       "0  We are a people-centric company dedicated towa...   \n",
       "0  Global Venture Captial and Private Equity/Busi...   \n",
       "0  Director @Deloitte. Long-time expert in #DataS...   \n",
       "\n",
       "               user_created  user_followers  user_friends  user_favourites  \\\n",
       "0 2021-08-11 19:50:54+00:00              47            20               54   \n",
       "0 2011-10-17 10:07:17+00:00             635           424            11177   \n",
       "0 2020-02-28 09:35:19+00:00            1968          1928                3   \n",
       "0 2015-05-18 20:52:29+00:00              60            11              824   \n",
       "0 2009-06-21 21:04:32+00:00           17923          1849             1472   \n",
       "\n",
       "   user_verified                      date  \\\n",
       "0          False 2022-02-25 11:43:18+00:00   \n",
       "0          False 2022-02-25 11:42:08+00:00   \n",
       "0          False 2022-02-25 11:41:23+00:00   \n",
       "0          False 2022-02-25 11:40:58+00:00   \n",
       "0          False 2022-02-25 11:40:01+00:00   \n",
       "\n",
       "                                                text  \\\n",
       "0  We're fundamentally changing how software deve...   \n",
       "0  Practical Advice for R in Production -  Answer...   \n",
       "0  Data can be simply defined as 'what you need t...   \n",
       "0  Daily Confirmed Covid Cases per 1K Population ...   \n",
       "0  Design Patterns in Machine Learning for MLOps ...   \n",
       "\n",
       "                                           hashtags           source  \\\n",
       "0            [softwareengineering, datascience, AI]  Twitter Web App   \n",
       "0  [Analytics, DataScience, AI, ML, RStats, Python]      NadarSenpai   \n",
       "0                                              None  Twitter Web App   \n",
       "0                                              None   smcapplication   \n",
       "0                 [analytics, datascience, bigdata]           Buffer   \n",
       "\n",
       "   is_retweet  \n",
       "0       False  \n",
       "0       False  \n",
       "0       False  \n",
       "0       False  \n",
       "0       False  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89d631f",
   "metadata": {
    "papermill": {
     "duration": 0.270304,
     "end_time": "2022-02-25T11:45:59.107087",
     "exception": false,
     "start_time": "2022-02-25T11:45:58.836783",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 5. Save the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80917965",
   "metadata": {
    "papermill": {
     "duration": 0.274594,
     "end_time": "2022-02-25T11:45:59.656080",
     "exception": false,
     "start_time": "2022-02-25T11:45:59.381486",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 5.1 Read past data\n",
    "\n",
    "##### Skip this part for the very first execution as there is no past data. Instead save your dataframe directly to a csv and use this part for the next runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2368b7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T11:46:00.203466Z",
     "iopub.status.busy": "2022-02-25T11:46:00.202788Z",
     "iopub.status.idle": "2022-02-25T11:46:00.223272Z",
     "shell.execute_reply": "2022-02-25T11:46:00.223906Z",
     "shell.execute_reply.started": "2021-10-17T08:33:48.595897Z"
    },
    "papermill": {
     "duration": 0.293592,
     "end_time": "2022-02-25T11:46:00.224126",
     "exception": false,
     "start_time": "2022-02-25T11:45:59.930534",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "past tweets: (500, 13)\n"
     ]
    }
   ],
   "source": [
    "tweets_old_df = pd.read_csv(\"../input/data-scraping-data-science-tweets/datascience_tweets.csv\")\n",
    "\n",
    "print(f\"past tweets: {tweets_old_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddb4de1",
   "metadata": {
    "papermill": {
     "duration": 0.270711,
     "end_time": "2022-02-25T11:46:00.765595",
     "exception": false,
     "start_time": "2022-02-25T11:46:00.494884",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 5.2 Merge Past and Present Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63d9ecae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T11:46:01.383929Z",
     "iopub.status.busy": "2022-02-25T11:46:01.383046Z",
     "iopub.status.idle": "2022-02-25T11:46:01.396495Z",
     "shell.execute_reply": "2022-02-25T11:46:01.397112Z",
     "shell.execute_reply.started": "2021-10-17T08:33:48.637196Z"
    },
    "papermill": {
     "duration": 0.2887,
     "end_time": "2022-02-25T11:46:01.397283",
     "exception": false,
     "start_time": "2022-02-25T11:46:01.108583",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new tweets: 500 past tweets: 500 all tweets: 1000\n"
     ]
    }
   ],
   "source": [
    "tweets_all_df = pd.concat([tweets_old_df, tweets_df], axis=0)\n",
    "\n",
    "print(f\"new tweets: {tweets_df.shape[0]} past tweets: {tweets_old_df.shape[0]} all tweets: {tweets_all_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db22bba",
   "metadata": {
    "papermill": {
     "duration": 0.266257,
     "end_time": "2022-02-25T11:46:01.935785",
     "exception": false,
     "start_time": "2022-02-25T11:46:01.669528",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 5.3 Drop Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de4759cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T11:46:02.479698Z",
     "iopub.status.busy": "2022-02-25T11:46:02.478866Z",
     "iopub.status.idle": "2022-02-25T11:46:02.496927Z",
     "shell.execute_reply": "2022-02-25T11:46:02.497328Z",
     "shell.execute_reply.started": "2021-10-17T08:33:48.655865Z"
    },
    "papermill": {
     "duration": 0.291341,
     "end_time": "2022-02-25T11:46:02.497482",
     "exception": false,
     "start_time": "2022-02-25T11:46:02.206141",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all tweets: (1000, 13)\n"
     ]
    }
   ],
   "source": [
    "tweets_all_df.drop_duplicates(subset = [\"user_name\", \"date\", \"text\"], inplace=True)\n",
    "print(f\"all tweets: {tweets_all_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0143e7",
   "metadata": {
    "papermill": {
     "duration": 0.172241,
     "end_time": "2022-02-25T11:46:02.843069",
     "exception": false,
     "start_time": "2022-02-25T11:46:02.670828",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 5.4 Export the updated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c00b5f8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T11:46:03.202179Z",
     "iopub.status.busy": "2022-02-25T11:46:03.201694Z",
     "iopub.status.idle": "2022-02-25T11:46:03.219592Z",
     "shell.execute_reply": "2022-02-25T11:46:03.219984Z",
     "shell.execute_reply.started": "2021-10-17T08:33:48.675102Z"
    },
    "papermill": {
     "duration": 0.200371,
     "end_time": "2022-02-25T11:46:03.220129",
     "exception": false,
     "start_time": "2022-02-25T11:46:03.019758",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tweets_all_df.to_csv(\"datascience_tweets.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c9b041",
   "metadata": {
    "papermill": {
     "duration": 0.177485,
     "end_time": "2022-02-25T11:46:03.574144",
     "exception": false,
     "start_time": "2022-02-25T11:46:03.396659",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Method 2 - Tweet Extractor using Twitter username\n",
    "\n",
    "##### Extract tweets of a particular user using the screen_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb246252",
   "metadata": {
    "papermill": {
     "duration": 0.175502,
     "end_time": "2022-02-25T11:46:03.927258",
     "exception": false,
     "start_time": "2022-02-25T11:46:03.751756",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e7a57ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T11:46:04.285536Z",
     "iopub.status.busy": "2022-02-25T11:46:04.284784Z",
     "iopub.status.idle": "2022-02-25T11:46:04.287911Z",
     "shell.execute_reply": "2022-02-25T11:46:04.288275Z",
     "shell.execute_reply.started": "2021-10-17T08:33:48.70396Z"
    },
    "papermill": {
     "duration": 0.183191,
     "end_time": "2022-02-25T11:46:04.288431",
     "exception": false,
     "start_time": "2022-02-25T11:46:04.105240",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bcc71b",
   "metadata": {
    "papermill": {
     "duration": 0.176397,
     "end_time": "2022-02-25T11:46:04.638290",
     "exception": false,
     "start_time": "2022-02-25T11:46:04.461893",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### 2. Twitter Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1fd61862",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T11:46:04.996980Z",
     "iopub.status.busy": "2022-02-25T11:46:04.991006Z",
     "iopub.status.idle": "2022-02-25T11:46:06.318031Z",
     "shell.execute_reply": "2022-02-25T11:46:06.317606Z",
     "shell.execute_reply.started": "2021-10-17T08:33:48.708287Z"
    },
    "papermill": {
     "duration": 1.506374,
     "end_time": "2022-02-25T11:46:06.318144",
     "exception": false,
     "start_time": "2022-02-25T11:46:04.811770",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Pass in the below parameters from your Twitter Developer account\n",
    "access_key = user_secrets.get_secret(\"ACCESS_KEY\")\n",
    "access_secret = user_secrets.get_secret(\"ACCESS_SECRET\")\n",
    "consumer_key = user_secrets.get_secret(\"CONSUMER_API_KEY\")\n",
    "consumer_secret = user_secrets.get_secret(\"CONSUMER_API_SECRET\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed0bb498",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T11:46:06.678884Z",
     "iopub.status.busy": "2022-02-25T11:46:06.678171Z",
     "iopub.status.idle": "2022-02-25T11:46:06.681466Z",
     "shell.execute_reply": "2022-02-25T11:46:06.680935Z",
     "shell.execute_reply.started": "2021-10-17T08:33:49.99517Z"
    },
    "papermill": {
     "duration": 0.186121,
     "end_time": "2022-02-25T11:46:06.681577",
     "exception": false,
     "start_time": "2022-02-25T11:46:06.495456",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_key, access_secret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5db92cb9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T11:46:07.051314Z",
     "iopub.status.busy": "2022-02-25T11:46:07.050757Z",
     "iopub.status.idle": "2022-02-25T11:46:07.054626Z",
     "shell.execute_reply": "2022-02-25T11:46:07.054038Z",
     "shell.execute_reply.started": "2021-10-17T08:33:50.000743Z"
    },
    "papermill": {
     "duration": 0.194726,
     "end_time": "2022-02-25T11:46:07.054746",
     "exception": false,
     "start_time": "2022-02-25T11:46:06.860020",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_all_tweets(screen_name):\n",
    "    alltweets = []\n",
    "    new_tweets = api.user_timeline(screen_name = screen_name,count=200) #Using the Twitter user_timeline API\n",
    "    alltweets.extend(new_tweets)\n",
    "    oldest = alltweets[-1].id - 1\n",
    "    \n",
    "    while len(new_tweets) > 0:\n",
    "        print(\"getting tweets before %s\" % (oldest))\n",
    "        new_tweets = api.user_timeline(screen_name = screen_name,count=200,max_id=oldest)\n",
    "        alltweets.extend(new_tweets)\n",
    "        oldest = alltweets[-1].id - 1\n",
    "        print (\"...%s tweets downloaded so far\" % (len(alltweets)))\n",
    "\n",
    "        data=[[obj.user.screen_name,obj.user.name,obj.user.id_str,obj.user.description.encode(\"utf8\"),obj.created_at.year,obj.created_at.month,obj.created_at.day,\"%s.%s\"%(obj.created_at.hour,obj.created_at.minute),obj.id_str,obj.text.encode(\"utf8\")] for obj in alltweets ]\n",
    "        dataframe=pd.DataFrame(data,columns=['screen_name','name','twitter_id','description','year','month','date','time','tweet_id','tweet'])\n",
    "        dataframe.to_csv(\"%s_tweets.csv\"%(screen_name),index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337f34e6",
   "metadata": {
    "papermill": {
     "duration": 0.17441,
     "end_time": "2022-02-25T11:46:07.402525",
     "exception": false,
     "start_time": "2022-02-25T11:46:07.228115",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Pass in the username of the account you want to download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e07dc68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T11:46:07.763890Z",
     "iopub.status.busy": "2022-02-25T11:46:07.763298Z",
     "iopub.status.idle": "2022-02-25T11:46:20.556065Z",
     "shell.execute_reply": "2022-02-25T11:46:20.555359Z",
     "shell.execute_reply.started": "2021-10-17T08:33:50.015289Z"
    },
    "papermill": {
     "duration": 12.97645,
     "end_time": "2022-02-25T11:46:20.556208",
     "exception": false,
     "start_time": "2022-02-25T11:46:07.579758",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting tweets before 1463382290856243201\n",
      "...399 tweets downloaded so far\n",
      "getting tweets before 1445090973772562431\n",
      "...598 tweets downloaded so far\n",
      "getting tweets before 1423771357347778561\n",
      "...798 tweets downloaded so far\n",
      "getting tweets before 1402719410637389825\n",
      "...998 tweets downloaded so far\n",
      "getting tweets before 1360720695337000961\n",
      "...1197 tweets downloaded so far\n",
      "getting tweets before 1318724213432254465\n",
      "...1397 tweets downloaded so far\n",
      "getting tweets before 1284348041084854271\n",
      "...1596 tweets downloaded so far\n",
      "getting tweets before 1267980322492125183\n",
      "...1795 tweets downloaded so far\n",
      "getting tweets before 1249880517966589951\n",
      "...1994 tweets downloaded so far\n",
      "getting tweets before 1229864795152666625\n",
      "...2194 tweets downloaded so far\n",
      "getting tweets before 1204634949296410624\n",
      "...2393 tweets downloaded so far\n",
      "getting tweets before 1187904787976773631\n",
      "...2589 tweets downloaded so far\n",
      "getting tweets before 1155984590663647232\n",
      "...2787 tweets downloaded so far\n",
      "getting tweets before 1131586670745141247\n",
      "...2983 tweets downloaded so far\n",
      "getting tweets before 1109028973864869887\n",
      "...3182 tweets downloaded so far\n",
      "getting tweets before 1098405820490891268\n",
      "...3232 tweets downloaded so far\n",
      "getting tweets before 1095465039438393343\n",
      "...3232 tweets downloaded so far\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\tget_all_tweets(\"jack\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340e0914",
   "metadata": {
    "papermill": {
     "duration": 0.275311,
     "end_time": "2022-02-25T11:46:21.107889",
     "exception": false,
     "start_time": "2022-02-25T11:46:20.832578",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Method 3 - Tweet Extractor using snscrape\n",
    "\n",
    "##### No Authentication required for using snscrape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0377c9ed",
   "metadata": {
    "papermill": {
     "duration": 0.277519,
     "end_time": "2022-02-25T11:46:21.666170",
     "exception": false,
     "start_time": "2022-02-25T11:46:21.388651",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# snscrape requires Python version 3.8 but Kaggle notebooks run on version 3.7. To avoid the error on execution, the snscrape codes have been commented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3b3c0ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T11:46:22.032866Z",
     "iopub.status.busy": "2022-02-25T11:46:22.032352Z",
     "iopub.status.idle": "2022-02-25T11:46:22.035613Z",
     "shell.execute_reply": "2022-02-25T11:46:22.036346Z",
     "shell.execute_reply.started": "2021-10-17T08:34:03.094186Z"
    },
    "papermill": {
     "duration": 0.185153,
     "end_time": "2022-02-25T11:46:22.036621",
     "exception": false,
     "start_time": "2022-02-25T11:46:21.851468",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4204e8",
   "metadata": {
    "papermill": {
     "duration": 0.181399,
     "end_time": "2022-02-25T11:46:22.398717",
     "exception": false,
     "start_time": "2022-02-25T11:46:22.217318",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Upgrade the verison of snscrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "718a3e97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T11:46:22.764844Z",
     "iopub.status.busy": "2022-02-25T11:46:22.764042Z",
     "iopub.status.idle": "2022-02-25T11:46:22.766905Z",
     "shell.execute_reply": "2022-02-25T11:46:22.767365Z",
     "shell.execute_reply.started": "2021-10-17T08:34:03.098407Z"
    },
    "papermill": {
     "duration": 0.186593,
     "end_time": "2022-02-25T11:46:22.767530",
     "exception": false,
     "start_time": "2022-02-25T11:46:22.580937",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install --upgrade git+https://github.com/JustAnotherArchivist/snscrape@master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "df25cf50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T11:46:23.130640Z",
     "iopub.status.busy": "2022-02-25T11:46:23.130093Z",
     "iopub.status.idle": "2022-02-25T11:46:23.132280Z",
     "shell.execute_reply": "2022-02-25T11:46:23.132642Z",
     "shell.execute_reply.started": "2021-10-17T08:34:03.112747Z"
    },
    "papermill": {
     "duration": 0.185071,
     "end_time": "2022-02-25T11:46:23.132758",
     "exception": false,
     "start_time": "2022-02-25T11:46:22.947687",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #Pass in the username whose tweets you want to pull\n",
    "#os.system(\"snscrape --jsonl twitter-search 'from:JeffBezos'> user-tweets.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "64ca0a2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T11:46:23.495488Z",
     "iopub.status.busy": "2022-02-25T11:46:23.494938Z",
     "iopub.status.idle": "2022-02-25T11:46:23.497886Z",
     "shell.execute_reply": "2022-02-25T11:46:23.498342Z",
     "shell.execute_reply.started": "2021-10-17T08:34:03.125037Z"
    },
    "papermill": {
     "duration": 0.187842,
     "end_time": "2022-02-25T11:46:23.498507",
     "exception": false,
     "start_time": "2022-02-25T11:46:23.310665",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "\n",
    "# # Reads the json generated from the CLI commands above and creates a pandas dataframe\n",
    "#tweets_df = pd.read_json('user-tweets.json', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774e36ca",
   "metadata": {
    "papermill": {
     "duration": 0.182584,
     "end_time": "2022-02-25T11:46:23.863518",
     "exception": false,
     "start_time": "2022-02-25T11:46:23.680934",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Check the shape and Info of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f3ac70e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T11:46:24.228055Z",
     "iopub.status.busy": "2022-02-25T11:46:24.227595Z",
     "iopub.status.idle": "2022-02-25T11:46:24.230758Z",
     "shell.execute_reply": "2022-02-25T11:46:24.231182Z",
     "shell.execute_reply.started": "2021-10-17T08:34:03.135598Z"
    },
    "papermill": {
     "duration": 0.186103,
     "end_time": "2022-02-25T11:46:24.231338",
     "exception": false,
     "start_time": "2022-02-25T11:46:24.045235",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#tweets_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "54c04a8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T11:46:24.598931Z",
     "iopub.status.busy": "2022-02-25T11:46:24.598303Z",
     "iopub.status.idle": "2022-02-25T11:46:24.602202Z",
     "shell.execute_reply": "2022-02-25T11:46:24.602787Z",
     "shell.execute_reply.started": "2021-10-17T08:34:03.145436Z"
    },
    "papermill": {
     "duration": 0.188147,
     "end_time": "2022-02-25T11:46:24.602980",
     "exception": false,
     "start_time": "2022-02-25T11:46:24.414833",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tweets_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddae11d",
   "metadata": {
    "papermill": {
     "duration": 0.283842,
     "end_time": "2022-02-25T11:46:25.172196",
     "exception": false,
     "start_time": "2022-02-25T11:46:24.888354",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Some more examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6707bc8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T11:46:25.738449Z",
     "iopub.status.busy": "2022-02-25T11:46:25.737713Z",
     "iopub.status.idle": "2022-02-25T11:46:25.740102Z",
     "shell.execute_reply": "2022-02-25T11:46:25.740638Z",
     "shell.execute_reply.started": "2021-10-17T08:34:03.156986Z"
    },
    "papermill": {
     "duration": 0.289007,
     "end_time": "2022-02-25T11:46:25.740799",
     "exception": false,
     "start_time": "2022-02-25T11:46:25.451792",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#os.system(\"snscrape --jsonl --max-results 100 twitter-search 'from:user'> user-tweets.json\")\n",
    "\n",
    "#os.system(\"snscrape --jsonl --max-results 500 --since 2020-06-01 twitter-search 'its the elephant until:2020-07-31' > text-query-tweets.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1de00a",
   "metadata": {
    "papermill": {
     "duration": 0.282385,
     "end_time": "2022-02-25T11:46:26.307066",
     "exception": false,
     "start_time": "2022-02-25T11:46:26.024681",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Creating a dataframe from the results of the examples above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "55a0ee9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T11:46:26.873748Z",
     "iopub.status.busy": "2022-02-25T11:46:26.872931Z",
     "iopub.status.idle": "2022-02-25T11:46:26.875347Z",
     "shell.execute_reply": "2022-02-25T11:46:26.874820Z",
     "shell.execute_reply.started": "2021-10-17T08:34:03.169681Z"
    },
    "papermill": {
     "duration": 0.284885,
     "end_time": "2022-02-25T11:46:26.875512",
     "exception": false,
     "start_time": "2022-02-25T11:46:26.590627",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import snscrape.modules.twitter as sntwitter\n",
    "# import pandas as pd\n",
    "\n",
    "# # Creating list to append tweet data to\n",
    "# tweets_list1 = []\n",
    "\n",
    "# # Using TwitterSearchScraper to scrape data and append tweets to list\n",
    "# for i,tweet in enumerate(sntwitter.TwitterSearchScraper('from:user').get_items()):\n",
    "#     if i>100:\n",
    "#         break\n",
    "#     tweets_list1.append([tweet.date, tweet.id, tweet.content, tweet.user.username])\n",
    "    \n",
    "# # Creating a dataframe from the tweets list above \n",
    "# tweets_df1 = pd.DataFrame(tweets_list1, columns=['Datetime', 'Tweet Id', 'Text', 'Username'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fa7f3b8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T11:46:27.441564Z",
     "iopub.status.busy": "2022-02-25T11:46:27.440879Z",
     "iopub.status.idle": "2022-02-25T11:46:27.442990Z",
     "shell.execute_reply": "2022-02-25T11:46:27.443651Z",
     "shell.execute_reply.started": "2021-10-17T08:34:03.187643Z"
    },
    "papermill": {
     "duration": 0.287412,
     "end_time": "2022-02-25T11:46:27.443824",
     "exception": false,
     "start_time": "2022-02-25T11:46:27.156412",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tweets_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3f0e8815",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T11:46:28.002446Z",
     "iopub.status.busy": "2022-02-25T11:46:28.001758Z",
     "iopub.status.idle": "2022-02-25T11:46:28.003681Z",
     "shell.execute_reply": "2022-02-25T11:46:28.004252Z",
     "shell.execute_reply.started": "2021-10-17T08:34:03.200747Z"
    },
    "papermill": {
     "duration": 0.283865,
     "end_time": "2022-02-25T11:46:28.004445",
     "exception": false,
     "start_time": "2022-02-25T11:46:27.720580",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Creating list to append tweet data to\n",
    "# tweets_list2 = []\n",
    "\n",
    "# # Using TwitterSearchScraper to scrape data and append tweets to list\n",
    "# for i,tweet in enumerate(sntwitter.TwitterSearchScraper('its the elephant since:2020-06-01 until:2020-07-31').get_items()):\n",
    "#     if i>500:\n",
    "#         break\n",
    "#     tweets_list2.append([tweet.date, tweet.id, tweet.content, tweet.user.username])\n",
    "    \n",
    "# # Creating a dataframe from the tweets list above\n",
    "# tweets_df2 = pd.DataFrame(tweets_list2, columns=['Datetime', 'Tweet Id', 'Text', 'Username'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ae29add8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T11:46:28.559353Z",
     "iopub.status.busy": "2022-02-25T11:46:28.558675Z",
     "iopub.status.idle": "2022-02-25T11:46:28.561201Z",
     "shell.execute_reply": "2022-02-25T11:46:28.561762Z",
     "shell.execute_reply.started": "2021-10-17T08:34:03.213449Z"
    },
    "papermill": {
     "duration": 0.283132,
     "end_time": "2022-02-25T11:46:28.561935",
     "exception": false,
     "start_time": "2022-02-25T11:46:28.278803",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tweets_df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd1128a",
   "metadata": {
    "papermill": {
     "duration": 0.275528,
     "end_time": "2022-02-25T11:46:29.114550",
     "exception": false,
     "start_time": "2022-02-25T11:46:28.839022",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data Scraping from Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4d7c707d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T11:46:29.676799Z",
     "iopub.status.busy": "2022-02-25T11:46:29.676115Z",
     "iopub.status.idle": "2022-02-25T11:46:36.995896Z",
     "shell.execute_reply": "2022-02-25T11:46:36.995165Z",
     "shell.execute_reply.started": "2021-10-17T08:34:03.225116Z"
    },
    "papermill": {
     "duration": 7.600061,
     "end_time": "2022-02-25T11:46:36.996010",
     "exception": false,
     "start_time": "2022-02-25T11:46:29.395949",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting praw\r\n",
      "  Downloading praw-7.5.0-py3-none-any.whl (176 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 176 kB 595 kB/s \r\n",
      "\u001b[?25hCollecting prawcore<3,>=2.1\r\n",
      "  Downloading prawcore-2.3.0-py3-none-any.whl (16 kB)\r\n",
      "Requirement already satisfied: update-checker>=0.18 in /opt/conda/lib/python3.7/site-packages (from praw) (0.18.0)\r\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in /opt/conda/lib/python3.7/site-packages (from praw) (0.57.0)\r\n",
      "Requirement already satisfied: requests<3.0,>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from prawcore<3,>=2.1->praw) (2.27.1)\r\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2.0.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2.10)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (1.26.6)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2021.5.30)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from websocket-client>=0.54.0->praw) (1.15.0)\r\n",
      "Installing collected packages: prawcore, praw\r\n",
      "Successfully installed praw-7.5.0 prawcore-2.3.0\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# !pip install praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c44e898a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T11:46:37.369468Z",
     "iopub.status.busy": "2022-02-25T11:46:37.368966Z",
     "iopub.status.idle": "2022-02-25T11:46:37.422165Z",
     "shell.execute_reply": "2022-02-25T11:46:37.422561Z",
     "shell.execute_reply.started": "2021-10-17T08:34:11.247262Z"
    },
    "papermill": {
     "duration": 0.242053,
     "end_time": "2022-02-25T11:46:37.422705",
     "exception": false,
     "start_time": "2022-02-25T11:46:37.180652",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import praw\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5862f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: user_agent in c:\\users\\jonch\\code\\scrape-social-medias\\venv\\lib\\site-packages (0.1.10)\n",
      "Requirement already satisfied: six in c:\\users\\jonch\\code\\scrape-social-medias\\venv\\lib\\site-packages (from user_agent) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 22.3 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\jonch\\code\\scrape-social-medias\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# !pip install user_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a9eb103",
   "metadata": {},
   "outputs": [],
   "source": [
    "from user_agent import generate_user_agent, generate_navigator\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c2b4310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:45.0) Gecko/20100101 '\n",
      " 'Firefox/45.0')\n",
      "'Mozilla/5.0 (X11; Linux x86_64; rv:50.0) Gecko/20100101 Firefox/50.0'\n"
     ]
    }
   ],
   "source": [
    "a_user_agent = generate_user_agent()\n",
    "pprint(a_user_agent)\n",
    "\n",
    "import config_reddit\n",
    "\n",
    "# Script App\n",
    "\n",
    "CLIENT_ID=config_reddit.CLIENT_ID\n",
    "CLIENT_SECRET=config_reddit.CLIENT_SECRET\n",
    "USER_AGENT=a_user_agent # \"testscript by u/jonc2000\"\n",
    "USERNAME=config_reddit.USERNAME\n",
    "PASSWORD=config_reddit.PASSWORD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03f24ba5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T11:46:37.788463Z",
     "iopub.status.busy": "2022-02-25T11:46:37.787947Z",
     "iopub.status.idle": "2022-02-25T11:46:37.792183Z",
     "shell.execute_reply": "2022-02-25T11:46:37.792659Z",
     "shell.execute_reply.started": "2021-10-17T08:34:11.297656Z"
    },
    "papermill": {
     "duration": 0.189454,
     "end_time": "2022-02-25T11:46:37.792795",
     "exception": false,
     "start_time": "2022-02-25T11:46:37.603341",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_date(created):\n",
    "    return dt.datetime.fromtimestamp(created)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96167e96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T11:46:38.156810Z",
     "iopub.status.busy": "2022-02-25T11:46:38.156266Z",
     "iopub.status.idle": "2022-02-25T11:46:38.162493Z",
     "shell.execute_reply": "2022-02-25T11:46:38.162955Z",
     "shell.execute_reply.started": "2021-10-17T08:34:11.306035Z"
    },
    "papermill": {
     "duration": 0.188995,
     "end_time": "2022-02-25T11:46:38.163079",
     "exception": false,
     "start_time": "2022-02-25T11:46:37.974084",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#fill in the below Authentication details from Reddit\n",
    "def reddit_connection():\n",
    "    personal_use_script = CLIENT_ID\n",
    "    client_secret = CLIENT_SECRET\n",
    "    user_agent = USER_AGENT\n",
    "    username = USERNAME\n",
    "    password = PASSWORD\n",
    "\n",
    "    reddit = praw.Reddit(client_id=personal_use_script, \\\n",
    "                         client_secret=client_secret, \\\n",
    "                         user_agent=user_agent, \\\n",
    "                         username=username, \\\n",
    "                         password='')\n",
    "    return reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a542590e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T11:46:38.526847Z",
     "iopub.status.busy": "2022-02-25T11:46:38.526304Z",
     "iopub.status.idle": "2022-02-25T11:46:38.538077Z",
     "shell.execute_reply": "2022-02-25T11:46:38.538534Z",
     "shell.execute_reply.started": "2021-10-17T08:34:11.315741Z"
    },
    "papermill": {
     "duration": 0.195877,
     "end_time": "2022-02-25T11:46:38.538672",
     "exception": false,
     "start_time": "2022-02-25T11:46:38.342795",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_dataset(reddit, search_words='stablediffusion', items_limit=100):\n",
    "\n",
    "    # Collect reddit posts\n",
    "    subreddit = reddit.subreddit(search_words)\n",
    "    new_subreddit = subreddit.new(limit=items_limit)\n",
    "    topics_dict = { \"title\":[],\n",
    "                \"score\":[],\n",
    "                \"id\":[], \"url\":[],\n",
    "                \"comms_num\": [],\n",
    "                \"created\": [],\n",
    "                \"body\":[]}\n",
    "\n",
    "    print(f\"retreive new reddit posts ...\")\n",
    "    for submission in tqdm(new_subreddit):\n",
    "        topics_dict[\"title\"].append(submission.title)\n",
    "        topics_dict[\"score\"].append(submission.score)\n",
    "        topics_dict[\"id\"].append(submission.id)\n",
    "        topics_dict[\"url\"].append(submission.url)\n",
    "        topics_dict[\"comms_num\"].append(submission.num_comments)\n",
    "        topics_dict[\"created\"].append(submission.created)\n",
    "        topics_dict[\"body\"].append(submission.selftext)\n",
    "\n",
    "    for comment in tqdm(subreddit.comments(limit=None)):\n",
    "        topics_dict[\"title\"].append(\"Comment\")\n",
    "        topics_dict[\"score\"].append(comment.score)\n",
    "        topics_dict[\"id\"].append(comment.id)\n",
    "        topics_dict[\"url\"].append(\"\")\n",
    "        topics_dict[\"comms_num\"].append(0)\n",
    "        topics_dict[\"created\"].append(comment.created)\n",
    "        topics_dict[\"body\"].append(comment.body)\n",
    "\n",
    "    topics_df = pd.DataFrame(topics_dict)\n",
    "    print(f\"new reddit posts retrieved: {len(topics_df)}\")\n",
    "    topics_df['timestamp'] = topics_df['created'].apply(lambda x: get_date(x))\n",
    "\n",
    "    return topics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ab7cc3d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T11:46:38.914794Z",
     "iopub.status.busy": "2022-02-25T11:46:38.914269Z",
     "iopub.status.idle": "2022-02-25T11:46:38.920527Z",
     "shell.execute_reply": "2022-02-25T11:46:38.920986Z",
     "shell.execute_reply.started": "2021-10-17T08:34:11.332774Z"
    },
    "papermill": {
     "duration": 0.19603,
     "end_time": "2022-02-25T11:46:38.921125",
     "exception": false,
     "start_time": "2022-02-25T11:46:38.725095",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def update_and_save_dataset(topics_df):   \n",
    "    file_path = \"reddit_stablediffusion.csv\"\n",
    "    if os.path.exists(file_path):\n",
    "        topics_old_df = pd.read_csv(file_path)\n",
    "        print(f\"past reddit posts: {topics_old_df.shape}\")\n",
    "        topics_all_df = pd.concat([topics_old_df, topics_df], axis=0)\n",
    "        print(f\"new reddit posts: {topics_df.shape[0]} past posts: {topics_old_df.shape[0]} all posts: {topics_all_df.shape[0]}\")\n",
    "        topics_new_df = topics_all_df.drop_duplicates(subset = [\"id\"], keep='last', inplace=False)\n",
    "        print(f\"all reddit posts: {topics_new_df.shape}\")\n",
    "        topics_new_df.to_csv(file_path, index=False)\n",
    "    else:\n",
    "        print(f\"reddit posts: {topics_df.shape}\")\n",
    "        topics_df.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c3033988",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T11:46:39.286647Z",
     "iopub.status.busy": "2022-02-25T11:46:39.286125Z",
     "iopub.status.idle": "2022-02-25T11:47:00.450080Z",
     "shell.execute_reply": "2022-02-25T11:47:00.450605Z",
     "shell.execute_reply.started": "2021-10-17T08:34:11.347888Z"
    },
    "papermill": {
     "duration": 21.347918,
     "end_time": "2022-02-25T11:47:00.450737",
     "exception": false,
     "start_time": "2022-02-25T11:46:39.102819",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retreive new reddit posts ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:02, 38.72it/s]\n",
      "995it [01:36, 10.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new reddit posts retrieved: 1095\n",
      "reddit posts: (1095, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# if __name__ == \"__main__\": \n",
    "reddit = reddit_connection()\n",
    "topics_data_df = build_dataset(reddit)\n",
    "update_and_save_dataset(topics_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e81e437c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1095 entries, 0 to 1094\n",
      "Data columns (total 8 columns):\n",
      " #   Column     Non-Null Count  Dtype         \n",
      "---  ------     --------------  -----         \n",
      " 0   title      1095 non-null   object        \n",
      " 1   score      1095 non-null   int64         \n",
      " 2   id         1095 non-null   object        \n",
      " 3   url        1095 non-null   object        \n",
      " 4   comms_num  1095 non-null   int64         \n",
      " 5   created    1095 non-null   float64       \n",
      " 6   body       1095 non-null   object        \n",
      " 7   timestamp  1095 non-null   datetime64[ns]\n",
      "dtypes: datetime64[ns](1), float64(1), int64(2), object(4)\n",
      "memory usage: 68.6+ KB\n"
     ]
    }
   ],
   "source": [
    "topics_data_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9f482098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>comms_num</th>\n",
       "      <th>created</th>\n",
       "      <th>body</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Use a custom ckpt file for dreamfusion?</td>\n",
       "      <td>2</td>\n",
       "      <td>ydpptd</td>\n",
       "      <td>https://www.reddit.com/r/StableDiffusion/comme...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.666761e+09</td>\n",
       "      <td>I'm looking for a way to turn my art made from...</td>\n",
       "      <td>2022-10-26 01:15:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cat Escapes Mirror Dimension</td>\n",
       "      <td>2</td>\n",
       "      <td>ydphr7</td>\n",
       "      <td>https://i.redd.it/1hms2z7e13w91.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>1.666761e+09</td>\n",
       "      <td></td>\n",
       "      <td>2022-10-26 01:03:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Running a video through img2img frame by frame...</td>\n",
       "      <td>1</td>\n",
       "      <td>ydpahq</td>\n",
       "      <td>https://youtu.be/JlO02se5Yw8</td>\n",
       "      <td>0</td>\n",
       "      <td>1.666760e+09</td>\n",
       "      <td></td>\n",
       "      <td>2022-10-26 00:51:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Are there AI Model Commissions?</td>\n",
       "      <td>2</td>\n",
       "      <td>ydp6r5</td>\n",
       "      <td>https://www.reddit.com/r/StableDiffusion/comme...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.666760e+09</td>\n",
       "      <td>Bit of a weird question, but do people take co...</td>\n",
       "      <td>2022-10-26 00:45:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Medium Format Film Portraits</td>\n",
       "      <td>0</td>\n",
       "      <td>ydp2d3</td>\n",
       "      <td>https://www.reddit.com/gallery/ydp2d3</td>\n",
       "      <td>0</td>\n",
       "      <td>1.666759e+09</td>\n",
       "      <td></td>\n",
       "      <td>2022-10-26 00:39:11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  score      id  \\\n",
       "0            Use a custom ckpt file for dreamfusion?      2  ydpptd   \n",
       "1                       Cat Escapes Mirror Dimension      2  ydphr7   \n",
       "2  Running a video through img2img frame by frame...      1  ydpahq   \n",
       "3                    Are there AI Model Commissions?      2  ydp6r5   \n",
       "4                       Medium Format Film Portraits      0  ydp2d3   \n",
       "\n",
       "                                                 url  comms_num       created  \\\n",
       "0  https://www.reddit.com/r/StableDiffusion/comme...          0  1.666761e+09   \n",
       "1                https://i.redd.it/1hms2z7e13w91.jpg          1  1.666761e+09   \n",
       "2                       https://youtu.be/JlO02se5Yw8          0  1.666760e+09   \n",
       "3  https://www.reddit.com/r/StableDiffusion/comme...          0  1.666760e+09   \n",
       "4              https://www.reddit.com/gallery/ydp2d3          0  1.666759e+09   \n",
       "\n",
       "                                                body           timestamp  \n",
       "0  I'm looking for a way to turn my art made from... 2022-10-26 01:15:51  \n",
       "1                                                    2022-10-26 01:03:06  \n",
       "2                                                    2022-10-26 00:51:52  \n",
       "3  Bit of a weird question, but do people take co... 2022-10-26 00:45:59  \n",
       "4                                                    2022-10-26 00:39:11  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_data_df.to_csv('reddit_stablediffusion_20221026.csv')\n",
    "topics_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4f58d64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T11:47:00.837182Z",
     "iopub.status.busy": "2022-02-25T11:47:00.836722Z",
     "iopub.status.idle": "2022-02-25T11:47:00.863467Z",
     "shell.execute_reply": "2022-02-25T11:47:00.862959Z",
     "shell.execute_reply.started": "2021-10-17T08:35:20.098224Z"
    },
    "papermill": {
     "duration": 0.219847,
     "end_time": "2022-02-25T11:47:00.863597",
     "exception": false,
     "start_time": "2022-02-25T11:47:00.643750",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'reddit_stablediffusion.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m'\u001b[39;49m\u001b[39mreddit_stablediffusion.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\jonch\\code\\scrape-social-medias\\venv\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\jonch\\code\\scrape-social-medias\\venv\\lib\\site-packages\\pandas\\util\\_decorators.py:317\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    312\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    313\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[0;32m    314\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    315\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(inspect\u001b[39m.\u001b[39mcurrentframe()),\n\u001b[0;32m    316\u001b[0m     )\n\u001b[1;32m--> 317\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\jonch\\code\\scrape-social-medias\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\jonch\\code\\scrape-social-medias\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\jonch\\code\\scrape-social-medias\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32mc:\\Users\\jonch\\code\\scrape-social-medias\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1729\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1727\u001b[0m     is_text \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   1728\u001b[0m     mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1729\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[0;32m   1730\u001b[0m     f,\n\u001b[0;32m   1731\u001b[0m     mode,\n\u001b[0;32m   1732\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1733\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1734\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1735\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   1736\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1737\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1738\u001b[0m )\n\u001b[0;32m   1739\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1740\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\jonch\\code\\scrape-social-medias\\venv\\lib\\site-packages\\pandas\\io\\common.py:857\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    852\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    853\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    855\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    856\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 857\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    858\u001b[0m             handle,\n\u001b[0;32m    859\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    860\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    861\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    862\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    863\u001b[0m         )\n\u001b[0;32m    864\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    865\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    866\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'reddit_stablediffusion.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('reddit_stablediffusion.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6112f81a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T11:47:01.260690Z",
     "iopub.status.busy": "2022-02-25T11:47:01.260087Z",
     "iopub.status.idle": "2022-02-25T11:47:01.277175Z",
     "shell.execute_reply": "2022-02-25T11:47:01.276717Z",
     "shell.execute_reply.started": "2021-10-17T08:35:20.123713Z"
    },
    "papermill": {
     "duration": 0.220298,
     "end_time": "2022-02-25T11:47:01.277293",
     "exception": false,
     "start_time": "2022-02-25T11:47:01.056995",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>comms_num</th>\n",
       "      <th>created</th>\n",
       "      <th>body</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[SPOILERS] Never watched GoT, but it seemed si...</td>\n",
       "      <td>0</td>\n",
       "      <td>t10uaa</td>\n",
       "      <td>https://i.redd.it/esaepbevhyj81.png</td>\n",
       "      <td>9</td>\n",
       "      <td>1.645785e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-02-25 10:28:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Spoilers] A Feast for Crows in the Series</td>\n",
       "      <td>0</td>\n",
       "      <td>t0zgyv</td>\n",
       "      <td>https://www.reddit.com/r/gameofthrones/comment...</td>\n",
       "      <td>4</td>\n",
       "      <td>1.645780e+09</td>\n",
       "      <td>Oh boy, I am listening to this on various audi...</td>\n",
       "      <td>2022-02-25 09:00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[NO SPOILERS] Where is Braavos?</td>\n",
       "      <td>0</td>\n",
       "      <td>t0xra1</td>\n",
       "      <td>https://www.reddit.com/r/gameofthrones/comment...</td>\n",
       "      <td>11</td>\n",
       "      <td>1.645773e+09</td>\n",
       "      <td>Do y'all think Braavos is representative of a ...</td>\n",
       "      <td>2022-02-25 07:08:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[NO SPOILERS] Game of Thrones display</td>\n",
       "      <td>9</td>\n",
       "      <td>t0ufod</td>\n",
       "      <td>https://www.reddit.com/r/gameofthrones/comment...</td>\n",
       "      <td>2</td>\n",
       "      <td>1.645762e+09</td>\n",
       "      <td>I just finished watching all 8 seasons of GoT...</td>\n",
       "      <td>2022-02-25 04:06:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Spoilers] I'm confused about something in S3</td>\n",
       "      <td>2</td>\n",
       "      <td>t0ts25</td>\n",
       "      <td>https://www.reddit.com/r/gameofthrones/comment...</td>\n",
       "      <td>4</td>\n",
       "      <td>1.645760e+09</td>\n",
       "      <td>So, when Jon, Ygritte and the other Wildlings ...</td>\n",
       "      <td>2022-02-25 03:34:08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  score      id  \\\n",
       "0  [SPOILERS] Never watched GoT, but it seemed si...      0  t10uaa   \n",
       "1         [Spoilers] A Feast for Crows in the Series      0  t0zgyv   \n",
       "2                    [NO SPOILERS] Where is Braavos?      0  t0xra1   \n",
       "3              [NO SPOILERS] Game of Thrones display      9  t0ufod   \n",
       "4      [Spoilers] I'm confused about something in S3      2  t0ts25   \n",
       "\n",
       "                                                 url  comms_num       created  \\\n",
       "0                https://i.redd.it/esaepbevhyj81.png          9  1.645785e+09   \n",
       "1  https://www.reddit.com/r/gameofthrones/comment...          4  1.645780e+09   \n",
       "2  https://www.reddit.com/r/gameofthrones/comment...         11  1.645773e+09   \n",
       "3  https://www.reddit.com/r/gameofthrones/comment...          2  1.645762e+09   \n",
       "4  https://www.reddit.com/r/gameofthrones/comment...          4  1.645760e+09   \n",
       "\n",
       "                                                body            timestamp  \n",
       "0                                                NaN  2022-02-25 10:28:09  \n",
       "1  Oh boy, I am listening to this on various audi...  2022-02-25 09:00:23  \n",
       "2  Do y'all think Braavos is representative of a ...  2022-02-25 07:08:23  \n",
       "3   I just finished watching all 8 seasons of GoT...  2022-02-25 04:06:33  \n",
       "4  So, when Jon, Ygritte and the other Wildlings ...  2022-02-25 03:34:08  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cdb27b34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T11:47:01.666234Z",
     "iopub.status.busy": "2022-02-25T11:47:01.665706Z",
     "iopub.status.idle": "2022-02-25T11:47:01.670044Z",
     "shell.execute_reply": "2022-02-25T11:47:01.670548Z",
     "shell.execute_reply.started": "2021-10-17T08:35:20.139613Z"
    },
    "papermill": {
     "duration": 0.196278,
     "end_time": "2022-02-25T11:47:01.670699",
     "exception": false,
     "start_time": "2022-02-25T11:47:01.474421",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1972, 8)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba65ba10",
   "metadata": {
    "papermill": {
     "duration": 0.201287,
     "end_time": "2022-02-25T11:47:02.065965",
     "exception": false,
     "start_time": "2022-02-25T11:47:01.864678",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Acknowledgements\n",
    "\n",
    "1. [Reddit Extract content](https://github.com/gabrielpreda/reddit_extract_content/blob/main/reddit_pfizer_vaccine.py)\n",
    "2. [Tweet Extractor](https://github.com/gabrielpreda/covid-19-tweets/blob/master/covid-19-tweets.ipynb)\n",
    "3. [Github Snscrape](https://github.com/MartinBeckUT/TwitterScraper/tree/master/snscrape)\n",
    "4. [Medium article Snscrape](https://medium.com/dataseries/how-to-scrape-millions-of-tweets-using-snscrape-195ee3594721)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 240.103174,
   "end_time": "2022-02-25T11:47:03.636890",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-02-25T11:43:03.533716",
   "version": "2.3.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "592dff02aedb101b8211647b7ca137c66c9fb9c2b0865ea58414e430688c8cb6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
